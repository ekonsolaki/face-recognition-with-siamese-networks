{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Siamese_final.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7xuHB74664nq","colab_type":"code","outputId":"c1f08dc5-640f-47ed-cfa9-d6d98f368b33","executionInfo":{"status":"ok","timestamp":1567935072566,"user_tz":-180,"elapsed":12485,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isnâ€™t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm() "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Gen RAM Free: 10.7 GB  | Proc size: 15.8 GB\n","GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6V8sZH3m7GDZ","colab_type":"code","colab":{}},"source":["import os\n","import time\n","\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from PIL import Image\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras import models, optimizers\n","from tensorflow.keras.layers import Reshape, Conv2D, MaxPooling2D, Flatten, Dense, Input, Lambda, Dropout\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from pathlib import Path\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDEBx8ti7QdK","colab_type":"code","colab":{}},"source":["class Recognizer:\n","\n","    def __init__(self):\n","        tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","        self.__IMG_DIMENSIONS = 128\n","\n","        self.__model: Model = None\n","\n","    def build_model(self, dr: 0.3, learning_rate=0.0001):\n","        \"\"\"\n","\n","        :param dr:\n","        :param learning_rate:\n","        :return:\n","        \"\"\"\n","\n","        input_shape = ((self.__IMG_DIMENSIONS ** 2) * 3,)\n","        convolution_shape = (self.__IMG_DIMENSIONS, self.__IMG_DIMENSIONS, 3)\n","        \n","                \n","        seq_conv_model = Sequential()\n","        \n","        seq_conv_model.add(Reshape(input_shape=input_shape, target_shape=convolution_shape))\n","        seq_conv_model.add(Conv2D(32, kernel_size=(4, 4), activation='relu'))\n","        seq_conv_model.add(Conv2D(32, kernel_size=(4, 4), activation='relu'))\n","        seq_conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n","        seq_conv_model.add(Dropout(dr))\n","        seq_conv_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n","        seq_conv_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n","        seq_conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n","        seq_conv_model.add(Dropout(dr))\n","        seq_conv_model.add(Flatten())\n","        seq_conv_model.add(Dense(128, activation='relu'))\n","        seq_conv_model.add(Dropout(0.5))\n","        seq_conv_model.add(Dense(64, activation='sigmoid'))\n","\n","        print(seq_conv_model.summary())\n","\n","        input_x1 = Input(shape=input_shape)\n","        input_x2 = Input(shape=input_shape)\n","\n","        output_x1 = seq_conv_model(input_x1)\n","        output_x2 = seq_conv_model(input_x2)\n","\n","        distance_l1 = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))([output_x1, output_x2])\n","\n","        outputs = Dense(1, activation='sigmoid')(distance_l1)\n","\n","        self.__model = Model([input_x1, input_x2], outputs)\n","\n","        self.__model.compile(loss='binary_crossentropy',\n","                             optimizer=optimizers.Adam(lr=learning_rate),\n","                             metrics=['binary_accuracy'])\n","\n","        self.__model.summary()\n","\n","        return self.__model\n","\n","    def prepare_images_from_dir(self, dir_path, flatten=True):\n","        \"\"\"\n","\n","        :param self:\n","        :param dir_path:\n","        :param flatten:\n","        :return:\n","        \"\"\"\n","        images = list()\n","        images_names = os.listdir(dir_path)\n","\n","        for imageName in images_names:\n","\n","            image = Image.open(dir_path + imageName)\n","            resize_image = image.resize((self.__IMG_DIMENSIONS, self.__IMG_DIMENSIONS))\n","            array = list()\n","            for x in range(self.__IMG_DIMENSIONS):\n","                sub_array = list()\n","                for y in range(self.__IMG_DIMENSIONS):\n","                    sub_array.append(resize_image.load()[x, y])\n","                array.append(sub_array)\n","            image_data = np.array(array)\n","            image = np.array(np.reshape(image_data, (self.__IMG_DIMENSIONS, self.__IMG_DIMENSIONS, 3)))\n","            images.append(image)\n","\n","        if flatten:\n","            images = np.array(images)\n","            return images.reshape((images.shape[0], self.__IMG_DIMENSIONS ** 2 * 3)).astype(np.float32)\n","        else:\n","            return np.array(images)\n","\n","    def fit(self, X, Y, hyper_parameters):\n","\n","        initial_time = time.time()\n","        self.__model.fit(X, Y,\n","                         batch_size=hyper_parameters['batch_size'],\n","                         epochs=hyper_parameters['epochs'],\n","                         callbacks=hyper_parameters['callbacks'],\n","                         validation_split=0.2,\n","                         verbose=1\n","                         )\n","        final_time = time.time()\n","        eta = (final_time - initial_time)\n","\n","        time_unit = 'seconds'\n","        if eta >= 60:\n","            eta = eta / 60\n","            time_unit = 'minutes'\n","\n","        print('Elapsed time acquired for {} epoch(s) -> {} {}'.format(hyper_parameters['epochs'], eta, time_unit))\n","\n","    def evaluate(self, test_X, test_Y):\n","        return self.__model.evaluate(test_X, test_Y)\n","\n","    def predict(self, X):\n","        predictions = self.__model.predict(X)\n","        return predictions\n","\n","    def summary(self):\n","        self.__model.summary()\n","\n","    def save_model(self, file_path):\n","        self.__model.save(file_path)\n","\n","    def load_model(self, file_path):\n","        self.__model = models.load_model(file_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMht1gKX7iT1","colab_type":"code","outputId":"6a9565c8-a700-439a-ffd3-ccd25bc28018","executionInfo":{"status":"ok","timestamp":1567934927346,"user_tz":-180,"elapsed":869,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XP49ttMT76XQ","colab_type":"code","colab":{}},"source":["def load_images_and_targets(img_dir, img_dimen=128):\n","    \"\"\"\n","\n","    :param img_dir:\n","    :param output_dir:\n","    :param img_dimen:\n","    :return:\n","    \"\"\"\n","    sub_dir_list = os.listdir(img_dir)\n","    images = list()\n","    targets = list()\n","\n","    \n","    for idx in tqdm(range(len(sub_dir_list))):\n","\n","        label = idx\n","        image_names = os.listdir(dir_path.joinpath(sub_dir_list[idx]))\n","        for image_path in image_names:\n","            path = dir_path.joinpath(sub_dir_list[idx], image_path)\n","            try:\n","\n","                img = img_to_array(load_img(path, target_size=(img_dimen, img_dimen))) / 255.\n","                images.append(img)\n","                targets.append(label)\n","            except Exception as e:\n","                print('WARNING : File {} could not be processed.'.format(path))\n","                print(e)\n","\n","    images = np.array(images)\n","\n","    left_samples = list()\n","    right_samples = list()\n","    labels = list()\n","    \n","    print(images.shape)\n","\n","    for i in tqdm(range(len(targets))):\n","        for j in range(len(targets)):\n","            left_samples.append(images[i])\n","            right_samples.append(images[j])\n","\n","            left_target = targets[i]\n","            right_target = targets[j]\n","            # Yi = 1 ; if both images contain the same person\n","            # Yi = 0; if both images contain different people\n","            if left_target == right_target:\n","                labels.append(1)\n","            else:\n","                labels.append(0)\n","\n","    x_left = np.array(left_samples)\n","    x_right = np.array(right_samples)\n","    y = np.array(labels)\n","\n","    print('X left shape: {}'.format(x_left.shape))\n","    print('X right shape: {}'.format(x_right.shape))\n","    print('Y shape: {}'.format(y.shape))\n","\n","    return x_left, x_right, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zN3Y9c5r8Cvz","colab_type":"code","colab":{}},"source":["project_dir = Path('drive/My Drive/BDCA Assignment')\n","models_dir = project_dir.joinpath('models')\n","\n","IMG_DIMEN = 128\n","dir_path = project_dir.joinpath('images_trial')\n","out_path = project_dir.joinpath('output_images')\n","testing_dir = project_dir.joinpath('custom_images')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8gIe4y_s8UMG","colab_type":"code","outputId":"9ef7c595-eac2-4c57-a8b5-404fbf6d55b0","executionInfo":{"status":"ok","timestamp":1567943326509,"user_tz":-180,"elapsed":945,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["dir_path.exists()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"pCdck0-98YUA","colab_type":"code","outputId":"823ab3f5-7269-47be-8513-aa9ebb30bb0e","executionInfo":{"status":"ok","timestamp":1567943327555,"user_tz":-180,"elapsed":518,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["out_path.exists()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"I2C5l1DK8eB2","colab_type":"code","outputId":"44e95c8d-a55c-429c-a1cb-5589214d020b","executionInfo":{"status":"ok","timestamp":1567943330298,"user_tz":-180,"elapsed":929,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["models_dir.exists()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Z1e9k6zMuzFt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d20bd942-a7a2-4fee-e559-68ec6b171ca1","executionInfo":{"status":"ok","timestamp":1567943780261,"user_tz":-180,"elapsed":844,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}}},"source":["testing_dir.exists()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"lzOOGOYW9ayW","colab_type":"code","outputId":"65d3e8a8-244b-4b16-8756-b948738428e8","executionInfo":{"status":"ok","timestamp":1567934951300,"user_tz":-180,"elapsed":6949,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["X1, X2, Y = load_images_and_targets(img_dir=dir_path,\n","                                    img_dimen=IMG_DIMEN)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 119.18it/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:00<00:00, 6874.60it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["(198, 128, 128, 3)\n","X left shape: (39204, 128, 128, 3)\n","X right shape: (39204, 128, 128, 3)\n","Y shape: (39204,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ez3RVAsCFja-","colab_type":"code","colab":{}},"source":["np.save('{}/x1.npy'.format(out_path), X1)\n","np.save('{}/x2.npy'.format(out_path), X2)\n","np.save('{}/y.npy'.format(out_path), Y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnwWTXEoFpXL","colab_type":"code","outputId":"544825f2-c3a2-4bec-8925-306f5a3ac30f","executionInfo":{"status":"ok","timestamp":1567935212375,"user_tz":-180,"elapsed":96109,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["X1 = np.load('{}/x1.npy'.format(out_path))\n","X2 = np.load('{}/x2.npy'.format(out_path))\n","Y = np.load('{}/y.npy'.format(out_path))\n","\n","X1 = X1.reshape((X1.shape[0], IMG_DIMEN ** 2 * 3)).astype(np.float32)\n","X2 = X2.reshape((X2.shape[0], IMG_DIMEN ** 2 * 3)).astype(np.float32)\n","\n","print(X1.shape)\n","print(X2.shape)\n","print(Y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(39204, 49152)\n","(39204, 49152)\n","(39204,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBKkD9VcFy2_","colab_type":"code","colab":{}},"source":["recognizer = Recognizer()\n","# recognizer.load_model('models/model.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBpdhGTIFzT_","colab_type":"code","colab":{}},"source":["callbacks = [\n","    EarlyStopping(monitor='val_loss',\n","                  patience=2,\n","                  verbose=1)\n","\n","]\n","\n","parameters = {\n","    'batch_size': 256,\n","    'epochs': 50,\n","    'callbacks': callbacks\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Up6fwjAlF4UN","colab_type":"code","outputId":"3c804458-7430-49da-a5e3-d4ad09797ae9","executionInfo":{"status":"ok","timestamp":1567937471816,"user_tz":-180,"elapsed":1127,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":988}},"source":["recognizer.build_model(dr=0.4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","reshape_2 (Reshape)          (None, 128, 128, 3)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 125, 125, 32)      1568      \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 122, 122, 32)      16416     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 61, 61, 32)        0         \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 61, 61, 32)        0         \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 59, 59, 64)        18496     \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 57, 57, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 50176)             0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 128)               6422656   \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 64)                8256      \n","=================================================================\n","Total params: 6,504,320\n","Trainable params: 6,504,320\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 49152)]      0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            [(None, 49152)]      0                                            \n","__________________________________________________________________________________________________\n","sequential_2 (Sequential)       (None, 64)           6504320     input_5[0][0]                    \n","                                                                 input_6[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 64)           0           sequential_2[1][0]               \n","                                                                 sequential_2[2][0]               \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 1)            65          lambda_2[0][0]                   \n","==================================================================================================\n","Total params: 6,504,385\n","Trainable params: 6,504,385\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.engine.training.Model at 0x7f91fa68a400>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"lyDNCNmUF7i2","colab_type":"code","outputId":"d9a7f994-193c-42bd-a985-f0657e44fc11","executionInfo":{"status":"ok","timestamp":1567941614891,"user_tz":-180,"elapsed":1074352,"user":{"displayName":"Eleni Konsolaki","photoUrl":"","userId":"05938860630161840148"}},"colab":{"base_uri":"https://localhost:8080/","height":817}},"source":["recognizer.fit(X=[X1, X2],\n","               Y=Y,\n","               hyper_parameters=parameters)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 31363 samples, validate on 7841 samples\n","Epoch 1/50\n","31363/31363 [==============================] - 156s 5ms/sample - loss: 0.1304 - binary_accuracy: 0.9744 - val_loss: 0.3301 - val_binary_accuracy: 0.9796\n","Epoch 2/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1286 - binary_accuracy: 0.9744 - val_loss: 0.3172 - val_binary_accuracy: 0.9796\n","Epoch 3/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.1273 - binary_accuracy: 0.9744 - val_loss: 0.3207 - val_binary_accuracy: 0.9796\n","Epoch 4/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1264 - binary_accuracy: 0.9744 - val_loss: 0.2996 - val_binary_accuracy: 0.9796\n","Epoch 5/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1248 - binary_accuracy: 0.9744 - val_loss: 0.2495 - val_binary_accuracy: 0.9796\n","Epoch 6/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.1240 - binary_accuracy: 0.9744 - val_loss: 0.1774 - val_binary_accuracy: 0.9796\n","Epoch 7/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1211 - binary_accuracy: 0.9744 - val_loss: 0.1574 - val_binary_accuracy: 0.9796\n","Epoch 8/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1181 - binary_accuracy: 0.9744 - val_loss: 0.1482 - val_binary_accuracy: 0.9796\n","Epoch 9/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1143 - binary_accuracy: 0.9744 - val_loss: 0.1306 - val_binary_accuracy: 0.9796\n","Epoch 10/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1096 - binary_accuracy: 0.9744 - val_loss: 0.1212 - val_binary_accuracy: 0.9796\n","Epoch 11/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.1051 - binary_accuracy: 0.9744 - val_loss: 0.1168 - val_binary_accuracy: 0.9796\n","Epoch 12/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.1010 - binary_accuracy: 0.9744 - val_loss: 0.1131 - val_binary_accuracy: 0.9796\n","Epoch 13/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.0956 - binary_accuracy: 0.9743 - val_loss: 0.1071 - val_binary_accuracy: 0.9796\n","Epoch 14/50\n","31363/31363 [==============================] - 150s 5ms/sample - loss: 0.0912 - binary_accuracy: 0.9745 - val_loss: 0.1010 - val_binary_accuracy: 0.9796\n","Epoch 15/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0867 - binary_accuracy: 0.9747 - val_loss: 0.0965 - val_binary_accuracy: 0.9796\n","Epoch 16/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0808 - binary_accuracy: 0.9750 - val_loss: 0.0928 - val_binary_accuracy: 0.9796\n","Epoch 17/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0769 - binary_accuracy: 0.9758 - val_loss: 0.0911 - val_binary_accuracy: 0.9796\n","Epoch 18/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0728 - binary_accuracy: 0.9769 - val_loss: 0.0855 - val_binary_accuracy: 0.9796\n","Epoch 19/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0689 - binary_accuracy: 0.9785 - val_loss: 0.0838 - val_binary_accuracy: 0.9796\n","Epoch 20/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0660 - binary_accuracy: 0.9797 - val_loss: 0.0853 - val_binary_accuracy: 0.9796\n","Epoch 21/50\n","31363/31363 [==============================] - 151s 5ms/sample - loss: 0.0628 - binary_accuracy: 0.9810 - val_loss: 0.0848 - val_binary_accuracy: 0.9796\n","Epoch 00021: early stopping\n","Elapsed time acquired for 50 epoch(s) -> 52.7981095790863 minutes\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jcWGZ2NSInjB","colab_type":"code","colab":{}},"source":["scores = list ()\n","labels = list ()\n","for image in custom_images:\n","    label = list ()\n","    score = list ()\n","    for sample in class_1_images:\n","        image, sample = image.reshape((1, -1)), sample.reshape((1, -1))\n","        score.append(recognizer.predict([image, sample])[0])\n","        label.append(0)\n","    for sample in class_2_images:\n","        image, sample = image.reshape((1, -1)), sample.reshape((1, -1))\n","        score.append(recognizer.predict([image, sample])[0])\n","        label.append(1)\n","    labels.append(label)\n","    scores.append(score)\n","\n","scores = np.array(scores)\n","labels = np.array(labels)\n","\n","for i in range(custom_images.shape[0]):\n","    index = np.argmax(scores[i])\n","    label_ = labels[i][index]\n","    print('IMAGE {} is {} with confidence of {}'.format(i + 1, label_, scores[i][index][0]))"],"execution_count":0,"outputs":[]}]}